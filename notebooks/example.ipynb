{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "Python version: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ]\n",
      "\n",
      "CUDA available: True\n",
      "\n",
      "CUDA Device Details:\n",
      "  Device: NVIDIA GeForce RTX 4060 Ti\n",
      "  Total memory: 15.60 GB\n",
      "  CUDA capability: 8.9\n",
      "  Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "\n",
    "def check_cuda():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "\n",
    "    # Check if CUDA is available\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"\\nCUDA available: {cuda_available}\")\n",
    "\n",
    "    if cuda_available:\n",
    "        # Get current CUDA device\n",
    "        current_device = torch.cuda.current_device()\n",
    "        # Get device properties\n",
    "        device_props = torch.cuda.get_device_properties(current_device)\n",
    "\n",
    "        print(\"\\nCUDA Device Details:\")\n",
    "        print(f\"  Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "        print(f\"  Total memory: {device_props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  CUDA capability: {device_props.major}.{device_props.minor}\")\n",
    "        print(f\"  Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"\\nNo CUDA devices available\")\n",
    "\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "nucleotides = [\"**\", \"A\", \"T\", \"C\", \"G\", \"#\"]  # vocabulary\n",
    "\n",
    "\n",
    "def token2nucleotide(s):\n",
    "    return nucleotides[s]\n",
    "\n",
    "\n",
    "PRIME_LENGTH = 4  # give the model a random DNA primer to start\n",
    "num_seq = 2  # number of runs\n",
    "context_length = (\n",
    "    10000  # maximal length for the generated sequence (upper limit for the model is 131K)\n",
    ")\n",
    "\n",
    "# model can be downloaded from https://huggingface.co/lingxusb/megaDNA_updated/resolve/main/megaDNA_phage_145M.pt\n",
    "model_path = \"./checkpoints/megaDNA_phage_145M.pt\"  # model name\n",
    "device = \"cuda\"  # change this to 'cuda' if you use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spica/Repos/megaDNA_matrix/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer sequence: GCAT\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/9996 [00:00<?, ?it/s]/home/spica/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "100%|██████████████████████████████████████████████████████████████| 9996/9996 [03:41<00:00, 45.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer sequence: AGGG\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 9996/9996 [03:41<00:00, 45.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for j in range(num_seq):\n",
    "    # Load the pre-trained model\n",
    "    model = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # set the random DNA primer\n",
    "    primer_sequence = (\n",
    "        torch.tensor(np.random.choice(np.arange(1, 5), PRIME_LENGTH)).long().to(device)[None,]\n",
    "    )\n",
    "    primer_DNA = \"\".join(map(token2nucleotide, primer_sequence[0]))\n",
    "    print(f\"Primer sequence: {primer_DNA}\\n{'*' * 100}\")\n",
    "\n",
    "    # Generate a sequence using the model\n",
    "    seq_tokenized = model.generate(\n",
    "        primer_sequence, seq_len=context_length, temperature=0.95, filter_thres=0.0\n",
    "    )\n",
    "    generated_sequence = \"\".join(map(token2nucleotide, seq_tokenized.squeeze().cpu().int()))\n",
    "\n",
    "    # Split the generated sequence into contigs at the '#' character\n",
    "    contigs = generated_sequence.split(\"#\")\n",
    "\n",
    "    # Write the contigs to a .fna file\n",
    "    output_file_path = f\"generate_{1 + j}.fna\"\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        for idx, contig in enumerate(contigs):\n",
    "            if len(contig) > 0:\n",
    "                file.write(f\">contig_{idx}\\n{contig}\\n\")\n",
    "\n",
    "    # Clean up to free memory\n",
    "    del model, primer_sequence, generated_sequence\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
