{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME: /run/current-system/sw\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Python version: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ]\n",
      "\n",
      "CUDA available: True\n",
      "\n",
      "CUDA Device Details:\n",
      "  Device: NVIDIA GeForce RTX 4060 Ti\n",
      "  Total memory: 15.60 GB\n",
      "  CUDA capability: 8.9\n",
      "  Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()  # will load from .env file in the same directory\n",
    "\n",
    "# Then add this to check the environment:\n",
    "\n",
    "print(f\"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not set')}\")\n",
    "\n",
    "\n",
    "def check_cuda():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "\n",
    "    # Check if CUDA is available\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"\\nCUDA available: {cuda_available}\")\n",
    "\n",
    "    if cuda_available:\n",
    "        # Get current CUDA device\n",
    "        current_device = torch.cuda.current_device()\n",
    "        # Get device properties\n",
    "        device_props = torch.cuda.get_device_properties(current_device)\n",
    "\n",
    "        print(\"\\nCUDA Device Details:\")\n",
    "        print(f\"  Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "        print(f\"  Total memory: {device_props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  CUDA capability: {device_props.major}.{device_props.minor}\")\n",
    "        print(f\"  Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"\\nNo CUDA devices available\")\n",
    "\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spica/Repos/megaDNA_matrix/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MEGADNA(\n",
       "  (start_tokens): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
       "      (1): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
       "      (2): Parameter containing: [torch.float32 of size 196 (cuda:0)]\n",
       "  )\n",
       "  (token_embs): ModuleList(\n",
       "    (0): Embedding(6, 196)\n",
       "    (1): Sequential(\n",
       "      (0): Embedding(6, 196)\n",
       "      (1): Rearrange('... r d -> ... (r d)')\n",
       "      (2): LayerNorm((3136,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=3136, out_features=256, bias=True)\n",
       "      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Embedding(6, 196)\n",
       "      (1): Rearrange('... r d -> ... (r d)')\n",
       "      (2): LayerNorm((200704,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=200704, out_features=512, bias=True)\n",
       "      (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (transformers): ModuleList(\n",
       "    (0): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (norm): RMSNorm()\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=128, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): RMSNorm()\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (norm): RMSNorm()\n",
       "            (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=256, out_features=128, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): RMSNorm()\n",
       "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (norm): RMSNorm()\n",
       "            (to_q): Linear(in_features=196, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=196, out_features=128, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=196, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): RMSNorm()\n",
       "            (1): Linear(in_features=196, out_features=784, bias=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=784, out_features=196, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (to_next_transformer_projections): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Rearrange('b ... d -> b (...) d')\n",
       "      (1): Linear(in_features=512, out_features=16384, bias=True)\n",
       "      (2): Rearrange('b m (n d) -> (b m) n d', n=64)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Rearrange('b ... d -> b (...) d')\n",
       "      (1): Linear(in_features=256, out_features=3136, bias=True)\n",
       "      (2): Rearrange('b m (n d) -> (b m) n d', n=16)\n",
       "    )\n",
       "    (2): Identity()\n",
       "  )\n",
       "  (to_logits): Linear(in_features=196, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model can be downloaded from https://huggingface.co/lingxusb/megaDNA_updated/resolve/main/megaDNA_phage_145M.pt\n",
    "model_path = \"./checkpoints/megaDNA_phage_145M.pt\"  # model name\n",
    "device = \"cuda\"  # change this to 'cuda' if you use GPU\n",
    "\n",
    "model = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nucleotides = [\"**\", \"A\", \"T\", \"C\", \"G\", \"#\"]  # vocabulary\n",
    "\n",
    "\n",
    "def token2nucleotide(s):\n",
    "    return nucleotides[s]\n",
    "\n",
    "\n",
    "PRIME_LENGTH = 4  # give the model a random DNA primer to start\n",
    "num_seq = 2  # number of runs\n",
    "context_length = (\n",
    "    10000  # maximal length for the generated sequence (upper limit for the model is 131K)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(num_seq):\n",
    "    # Load the pre-trained model\n",
    "    model = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # set the random DNA primer\n",
    "    primer_sequence = (\n",
    "        torch.tensor(np.random.choice(np.arange(1, 5), PRIME_LENGTH)).long().to(device)[None,]\n",
    "    )\n",
    "    primer_DNA = \"\".join(map(token2nucleotide, primer_sequence[0]))\n",
    "    print(f\"Primer sequence: {primer_DNA}\\n{'*' * 100}\")\n",
    "\n",
    "    # Generate a sequence using the model\n",
    "    seq_tokenized = model.generate(\n",
    "        primer_sequence, seq_len=context_length, temperature=0.95, filter_thres=0.0\n",
    "    )\n",
    "    generated_sequence = \"\".join(map(token2nucleotide, seq_tokenized.squeeze().cpu().int()))\n",
    "\n",
    "    # Split the generated sequence into contigs at the '#' character\n",
    "    contigs = generated_sequence.split(\"#\")\n",
    "\n",
    "    # Write the contigs to a .fna file\n",
    "    output_file_path = f\"generate_{1 + j}.fna\"\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        for idx, contig in enumerate(contigs):\n",
    "            if len(contig) > 0:\n",
    "                file.write(f\">contig_{idx}\\n{contig}\\n\")\n",
    "\n",
    "    # Clean up to free memory\n",
    "    del model, primer_sequence, generated_sequence\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mutagenesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding and loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[ 0.0109, -0.0855,  0.0778,  ..., -0.0298,  0.0041,  0.0096],\n",
      "         [-0.0152,  0.8417, -0.6062,  ...,  0.1078,  0.3091, -0.2047]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>), tensor([[[-0.6653,  0.2984,  0.3873,  ...,  0.0366,  0.0596, -0.0057],\n",
      "         [-0.8001,  0.3894,  0.3046,  ..., -0.3062,  0.0349,  0.4216],\n",
      "         [-0.6077,  0.2663,  0.5808,  ..., -0.1424, -0.1995, -0.0598],\n",
      "         ...,\n",
      "         [-0.4043,  0.2778,  0.5528,  ...,  0.1454, -0.2260,  0.9171],\n",
      "         [-0.4392,  0.2235,  0.5886,  ...,  0.1115, -0.2188,  0.9542],\n",
      "         [-0.5363,  0.2499,  0.6020,  ...,  0.0849, -0.2252,  0.9662]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>), tensor([[[ 8.5990e-02,  9.8310e-02, -7.9113e-03,  ...,  5.1405e-01,\n",
      "           2.1096e-01, -9.6927e-03],\n",
      "         [ 4.5448e-02,  2.6922e-02,  2.8464e-02,  ...,  1.2249e-02,\n",
      "          -9.8482e-02,  6.3418e-02],\n",
      "         [ 2.6853e-03,  2.7896e-02, -5.7082e-02,  ...,  1.2375e-02,\n",
      "          -2.2628e-02,  6.4354e-02],\n",
      "         ...,\n",
      "         [-2.8637e-02,  5.8583e-02, -1.4022e-02,  ..., -5.8142e-02,\n",
      "          -5.0600e-02,  7.6538e-02],\n",
      "         [-6.9445e-03,  3.4662e-02, -2.8879e-03,  ..., -9.4843e-02,\n",
      "          -1.4244e-01,  7.2222e-02],\n",
      "         [-1.7253e-03, -2.2970e-02, -1.3950e-03,  ..., -7.3031e-02,\n",
      "           8.2748e-03,  4.9223e-02]],\n",
      "\n",
      "        [[ 8.5990e-02,  9.8310e-02, -7.9113e-03,  ...,  5.1405e-01,\n",
      "           2.1096e-01, -9.6927e-03],\n",
      "         [-7.5142e-03, -1.2614e-01,  4.1530e-02,  ..., -5.2341e-02,\n",
      "          -4.5138e-02,  7.2098e-02],\n",
      "         [ 3.8622e-02,  4.0227e-02, -5.8058e-03,  ..., -3.8799e-02,\n",
      "          -1.0874e-01,  6.1185e-02],\n",
      "         ...,\n",
      "         [-3.8204e-03, -3.3326e-02,  7.3857e-03,  ..., -9.3220e-02,\n",
      "          -1.2099e-01,  9.7617e-02],\n",
      "         [-3.8776e-02, -2.1935e-02,  1.5462e-02,  ..., -6.6207e-02,\n",
      "          -1.3873e-01,  5.7381e-02],\n",
      "         [ 5.6274e-02,  2.6933e-02,  4.7883e-02,  ..., -5.2429e-02,\n",
      "          -1.3051e-01,  1.6639e-02]],\n",
      "\n",
      "        [[ 8.5990e-02,  9.8310e-02, -7.9113e-03,  ...,  5.1405e-01,\n",
      "           2.1096e-01, -9.6927e-03],\n",
      "         [ 1.1081e-01,  2.0171e-02,  7.8863e-02,  ..., -4.2336e-02,\n",
      "           6.1207e-02,  3.5786e-02],\n",
      "         [-3.7847e-02,  1.2763e-02,  1.1889e-02,  ..., -7.0231e-02,\n",
      "           4.1234e-02,  3.1447e-02],\n",
      "         ...,\n",
      "         [-1.3644e-02, -4.6836e-02, -4.7691e-02,  ..., -1.3241e-03,\n",
      "          -6.7004e-02,  4.1254e-02],\n",
      "         [-6.5226e-02,  4.1318e-02, -1.6844e-03,  ..., -1.6086e-01,\n",
      "          -1.5210e-02,  8.2937e-02],\n",
      "         [ 4.0799e-02, -3.7372e-02, -2.3260e-02,  ..., -4.9526e-02,\n",
      "           2.6623e-02,  1.1462e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 8.5990e-02,  9.8310e-02, -7.9113e-03,  ...,  5.1405e-01,\n",
      "           2.1096e-01, -9.6927e-03],\n",
      "         [ 1.1144e-02,  1.1015e-02,  2.4748e-03,  ..., -1.9520e-01,\n",
      "          -1.3835e-01,  4.4045e-02],\n",
      "         [-7.4201e-03, -2.0815e-02, -2.4914e-02,  ..., -2.0161e-01,\n",
      "           1.4531e-01,  2.9475e-02],\n",
      "         ...,\n",
      "         [-4.2804e-02, -2.7071e-02, -2.4153e-02,  ..., -1.8908e-01,\n",
      "           2.0055e-02,  1.5330e-01],\n",
      "         [-4.8141e-02, -1.8954e-02,  8.8785e-04,  ..., -1.5508e-01,\n",
      "           4.4310e-02,  1.4942e-01],\n",
      "         [-2.2561e-02,  1.0313e-02,  2.9712e-03,  ..., -1.0784e-01,\n",
      "           1.7339e-02,  5.3257e-02]],\n",
      "\n",
      "        [[ 8.5990e-02,  9.8310e-02, -7.9113e-03,  ...,  5.1405e-01,\n",
      "           2.1096e-01, -9.6927e-03],\n",
      "         [ 6.3833e-03,  1.0898e-02,  3.4197e-03,  ..., -2.0666e-01,\n",
      "          -1.3811e-01,  4.8220e-02],\n",
      "         [-2.1030e-03, -1.6397e-02, -2.6896e-02,  ..., -2.1691e-01,\n",
      "           1.6152e-01,  2.2269e-02],\n",
      "         ...,\n",
      "         [-3.7977e-02, -2.6562e-02, -1.8745e-02,  ..., -1.8407e-01,\n",
      "           1.0896e-02,  1.5160e-01],\n",
      "         [-4.3273e-02, -2.3236e-02,  5.2824e-03,  ..., -1.5695e-01,\n",
      "           2.9357e-02,  1.5193e-01],\n",
      "         [-2.6156e-02,  5.0579e-03,  2.9289e-03,  ..., -1.0287e-01,\n",
      "          -7.4716e-03,  5.8975e-02]],\n",
      "\n",
      "        [[ 8.5990e-02,  9.8310e-02, -7.9113e-03,  ...,  5.1405e-01,\n",
      "           2.1096e-01, -9.6927e-03],\n",
      "         [ 1.6224e-02,  9.6298e-03,  1.3399e-03,  ..., -2.0610e-01,\n",
      "          -1.4580e-01,  4.0058e-02],\n",
      "         [-1.6829e-02, -2.1326e-02, -2.8780e-02,  ..., -2.0910e-01,\n",
      "           1.5200e-01,  2.4392e-02],\n",
      "         ...,\n",
      "         [-4.0367e-02, -2.7955e-02, -2.6781e-02,  ..., -1.8001e-01,\n",
      "           2.6920e-02,  1.5568e-01],\n",
      "         [-4.8650e-02, -2.2259e-02,  2.5017e-04,  ..., -1.6495e-01,\n",
      "           5.3247e-02,  1.5218e-01],\n",
      "         [-2.9297e-02,  3.9300e-03,  3.6841e-03,  ..., -1.0975e-01,\n",
      "           1.9076e-02,  5.9849e-02]]], device='cuda:0', grad_fn=<MulBackward0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spica/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# a random input sequence\n",
    "encoded_sequence = np.random.choice(np.arange(1, 5), 100)\n",
    "input_seq = torch.tensor(encoded_sequence).unsqueeze(0).to(device)\n",
    "\n",
    "# get embeddings\n",
    "embeddings = model(input_seq, return_value=\"embedding\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4495, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# output[0:3] stores embeddings from three transformer layers.\n",
    "\n",
    "# get model loss\n",
    "loss = model(input_seq, return_value=\"loss\")\n",
    "\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
