{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME: /run/current-system/sw\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Python version: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ]\n",
      "\n",
      "CUDA available: True\n",
      "\n",
      "CUDA Device Details:\n",
      "  Device: NVIDIA GeForce RTX 4060 Ti\n",
      "  Total memory: 15.60 GB\n",
      "  CUDA capability: 8.9\n",
      "  Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()  # will load from .env file in the same directory\n",
    "\n",
    "# Then add this to check the environment:\n",
    "\n",
    "print(f\"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not set')}\")\n",
    "\n",
    "\n",
    "def check_cuda():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "\n",
    "    # Check if CUDA is available\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"\\nCUDA available: {cuda_available}\")\n",
    "\n",
    "    if cuda_available:\n",
    "        # Get current CUDA device\n",
    "        current_device = torch.cuda.current_device()\n",
    "        # Get device properties\n",
    "        device_props = torch.cuda.get_device_properties(current_device)\n",
    "\n",
    "        print(\"\\nCUDA Device Details:\")\n",
    "        print(f\"  Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "        print(f\"  Total memory: {device_props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  CUDA capability: {device_props.major}.{device_props.minor}\")\n",
    "        print(f\"  Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"\\nNo CUDA devices available\")\n",
    "\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spica/Repos/megaDNA_matrix/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "# model can be downloaded from https://huggingface.co/lingxusb/megaDNA_updated/resolve/main/megaDNA_phage_145M.pt\n",
    "model_path = \"../checkpoints/megaDNA_phage_145M.pt\"  # model name\n",
    "device = \"cuda\"  # change this to 'cuda' if you use GPU\n",
    "\n",
    "model = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "fasta_file_path = \"../dataset/1Jan2025_genomes.fa\"\n",
    "sequences = list(SeqIO.parse(fasta_file_path, \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqRecord(seq=Seq('TTTGGTGGAGCTGGCGGGAGTTGAACCCGCGTCCGAAATTCCTACATACCATTT...CAT'), id='AY319521', name='AY319521', description='AY319521 Salmonella phage SopEPhi, complete sequence.', dbxrefs=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     32043.000000\n",
       "mean      60215.966607\n",
       "std       55524.968903\n",
       "min        1761.000000\n",
       "25%       33532.500000\n",
       "50%       44866.000000\n",
       "75%       67557.500000\n",
       "max      735411.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = [len(str(seq.seq)) for seq in sequences]\n",
    "summary = pd.Series(seq_lengths).describe()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset with length less than 131072, randomly sample 1000\n",
    "seq_subset = [seq for seq in sequences if len(str(seq.seq)) < 131072]\n",
    "seq_subset = random.sample(seq_subset, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the subset\n",
    "with open(\"../dataset/1Jan2025_genomes_subset.fa\", \"w\") as f:\n",
    "    for seq in seq_subset:\n",
    "        f.write(f\">{seq.description}\\n{str(seq.seq)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the subset\n",
    "seq_subset = list(SeqIO.parse(\"../dataset/1Jan2025_genomes_subset.fa\", \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/1000 sequences\n",
      "Processed 100/1000 sequences\n",
      "Processed 150/1000 sequences\n",
      "Processed 200/1000 sequences\n",
      "Processed 250/1000 sequences\n",
      "Processed 300/1000 sequences\n",
      "Processed 350/1000 sequences\n",
      "Processed 400/1000 sequences\n",
      "Processed 450/1000 sequences\n",
      "Processed 500/1000 sequences\n",
      "Processed 550/1000 sequences\n",
      "Processed 600/1000 sequences\n",
      "Processed 650/1000 sequences\n",
      "Processed 700/1000 sequences\n",
      "Processed 750/1000 sequences\n",
      "Processed 800/1000 sequences\n",
      "Processed 850/1000 sequences\n",
      "Processed 900/1000 sequences\n",
      "Processed 950/1000 sequences\n",
      "Processed 1000/1000 sequences\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1000,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Convert lists to numpy arrays\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m local_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_embeddings_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m middle_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(middle_embeddings_list)\n\u001b[1;32m     61\u001b[0m global_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(global_embeddings_list)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1000,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "def get_embeddings(sequence, model, device, max_seq_length=131072):\n",
    "    \"\"\"Get embeddings for a single sequence\n",
    "    Args:\n",
    "        sequence: Input DNA sequence\n",
    "        model: MEGADNA model\n",
    "        device: torch device (CPU/GPU)\n",
    "        max_seq_length: Maximum sequence length (default: 131072 from model dimensions 128*64*16)\n",
    "    Returns:\n",
    "        combined_emb: 964-dimensional embedding vector (on GPU)\n",
    "    \"\"\"\n",
    "    # Check sequence length\n",
    "    if len(sequence) > max_seq_length:\n",
    "        print(f\"Warning: Sequence length {len(sequence)} exceeds max length {max_seq_length}\")\n",
    "        sequence = sequence[:max_seq_length]\n",
    "\n",
    "    # Encode sequence\n",
    "    nt_vocab = [\"**\", \"A\", \"T\", \"C\", \"G\", \"#\"]\n",
    "    encoded = [0]  # Start token\n",
    "    for nucleotide in str(sequence):\n",
    "        if nucleotide in nt_vocab:\n",
    "            encoded.append(nt_vocab.index(nucleotide))\n",
    "        else:\n",
    "            encoded.append(1)\n",
    "    encoded.append(5)  # End token\n",
    "\n",
    "    input_seq = torch.tensor(encoded).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input_seq, return_value=\"embedding\")\n",
    "        local_emb = torch.mean(embeddings[0].squeeze(), dim=0)  # (512,)\n",
    "        middle_emb = torch.mean(torch.mean(embeddings[1].squeeze(), dim=0), dim=0)  # (256,)\n",
    "        global_emb = torch.mean(torch.mean(embeddings[2].squeeze(), dim=0), dim=0)  # (196,)\n",
    "        combined_emb = torch.cat([local_emb, middle_emb, global_emb])  # (964,)\n",
    "\n",
    "    return combined_emb\n",
    "\n",
    "\n",
    "def process_sequences(sequences, model, device, batch_size=50, max_seq_length=131072):\n",
    "    \"\"\"Process multiple sequences in batches\n",
    "    Args:\n",
    "        sequences: List of sequences to process\n",
    "        model: MEGADNA model\n",
    "        device: torch device (CPU/GPU)\n",
    "        batch_size: Number of sequences to process at once\n",
    "        max_seq_length: Maximum sequence length (default: 131072 from model dimensions 128*64*16)\n",
    "    Returns:\n",
    "        all_embeddings: Tensor of shape (n_sequences, 964) containing embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = torch.zeros((len(sequences), 964), device=device)\n",
    "\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch = sequences[i : i + batch_size]\n",
    "\n",
    "        for j, record in enumerate(batch):\n",
    "            try:\n",
    "                all_embeddings[i + j] = get_embeddings(record.seq, model, device, max_seq_length)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sequence {record.id}: {str(e)}\")\n",
    "\n",
    "        print(f\"Processed {min(i + batch_size, len(sequences))}/{len(sequences)} sequences\")\n",
    "\n",
    "    return all_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
