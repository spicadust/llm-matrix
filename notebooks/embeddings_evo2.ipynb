{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRITON_LIBCUDA_PATH: /nix/store/z8ac4sgxc4h86zfmlz7yi0kkv95wgz84-graphics-drivers/lib\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the path is set\n",
    "print(\"TRITON_LIBCUDA_PATH:\", os.getenv(\"TRITON_LIBCUDA_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spica/Repos/megaDNA_matrix/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 25/25 [00:00<00:00, 314.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra keys in state_dict: {'blocks.3.mixer.dense._extra_state', 'blocks.17.mixer.dense._extra_state', 'unembed.weight', 'blocks.24.mixer.dense._extra_state', 'blocks.10.mixer.dense._extra_state'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from evo2 import Evo2\n",
    "\n",
    "evo2_model = Evo2(\"evo2_1b_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "sequence = \"ACGTAG\"\n",
    "input_ids = (\n",
    "    torch.tensor(\n",
    "        evo2_model.tokenizer.tokenize(sequence),\n",
    "        dtype=torch.int,\n",
    "    )\n",
    "    .unsqueeze(0)\n",
    "    .to(\"cuda:0\")\n",
    ")\n",
    "\n",
    "layer_name = \"blocks.24.mlp.l3\"\n",
    "\n",
    "outputs, embeddings = evo2_model(input_ids, return_embeddings=True, layer_names=[layer_name])\n",
    "\n",
    "print(\"Embeddings shape: \", embeddings[layer_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_subset_30k = list(SeqIO.parse(\"../dataset/1Jan2025_genomes_subset_30k.fa\", \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1938/3316 [28:50<54:33,  2.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence PP599996 Yersinia phage vB_YpM_MHG7, complete genome.: CUDA out of memory. Tried to allocate 3.40 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.63 GiB is free. Including non-PyTorch memory, this process has 12.96 GiB memory in use. Of the allocated memory 6.72 GiB is allocated by PyTorch, and 4.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1939/3316 [28:51<48:58,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence PP599995 Yersinia phage vB_YpM_MHG54, complete genome.: CUDA out of memory. Tried to allocate 3.40 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.63 GiB is free. Including non-PyTorch memory, this process has 12.96 GiB memory in use. Of the allocated memory 6.72 GiB is allocated by PyTorch, and 4.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 1940/3316 [28:53<45:26,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence PP599994 Yersinia phage vB_YpM_MHG39, complete genome.: CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.66 GiB is free. Including non-PyTorch memory, this process has 12.93 GiB memory in use. Of the allocated memory 6.70 GiB is allocated by PyTorch, and 4.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 1946/3316 [29:08<53:59,  2.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence PP537607 Yersinia phage vB_YpM_MHG101, complete genome.: CUDA out of memory. Tried to allocate 3.41 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 12.72 GiB memory in use. Of the allocated memory 6.60 GiB is allocated by PyTorch, and 3.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1964/3316 [29:44<19:33,  1.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence PP516625 Yersinia phage vB_YpM_MHG38, complete genome.: CUDA out of memory. Tried to allocate 3.41 GiB. GPU 0 has a total capacity of 15.60 GiB of which 3.39 GiB is free. Including non-PyTorch memory, this process has 12.20 GiB memory in use. Of the allocated memory 6.43 GiB is allocated by PyTorch, and 3.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1966/3316 [29:51<49:48,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence PP516623 Yersinia phage vB_YpM_MDG94-186, complete genome.: CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.68 GiB is free. Including non-PyTorch memory, this process has 12.91 GiB memory in use. Of the allocated memory 6.70 GiB is allocated by PyTorch, and 4.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2131/3316 [36:53<1:01:46,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence NC_070916 Yersinia phage vB_YenM_06.16-2, complete genome.: CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.78 GiB is free. Including non-PyTorch memory, this process has 12.81 GiB memory in use. Of the allocated memory 6.69 GiB is allocated by PyTorch, and 4.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2132/3316 [36:54<49:55,  2.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence NC_070911 Salmonella phage BIS20, complete genome.: CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.79 GiB is free. Including non-PyTorch memory, this process has 12.80 GiB memory in use. Of the allocated memory 6.68 GiB is allocated by PyTorch, and 4.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 2158/3316 [38:23<55:10,  2.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence NC_049392 Escherichia phage ESSI2_ev239 genome assembly, chromosome: 1.: CUDA out of memory. Tried to allocate 3.34 GiB. GPU 0 has a total capacity of 15.60 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 12.26 GiB memory in use. Of the allocated memory 6.51 GiB is allocated by PyTorch, and 3.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2238/3316 [42:24<54:11,  3.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence NC_029003 Salmonella phage SEN1, complete genome.: CUDA out of memory. Tried to allocate 3.40 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.66 GiB is free. Including non-PyTorch memory, this process has 12.93 GiB memory in use. Of the allocated memory 6.71 GiB is allocated by PyTorch, and 4.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2254/3316 [43:28<1:00:42,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence NC_019932 Erwinia phage ENT90, complete genome.: CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.69 GiB is free. Including non-PyTorch memory, this process has 12.90 GiB memory in use. Of the allocated memory 6.68 GiB is allocated by PyTorch, and 4.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 2488/3316 [54:44<31:47,  2.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence LR597637 Escherichia phage ESSI2_ev239 genome assembly, chromosome: 1.: CUDA out of memory. Tried to allocate 3.34 GiB. GPU 0 has a total capacity of 15.60 GiB of which 3.19 GiB is free. Including non-PyTorch memory, this process has 12.40 GiB memory in use. Of the allocated memory 6.52 GiB is allocated by PyTorch, and 3.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2655/3316 [1:00:40<26:26,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence KT630644 Salmonella phage SEN1, complete genome.: CUDA out of memory. Tried to allocate 3.40 GiB. GPU 0 has a total capacity of 15.60 GiB of which 3.01 GiB is free. Including non-PyTorch memory, this process has 12.58 GiB memory in use. Of the allocated memory 6.58 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 3010/3316 [1:05:42<12:48,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence KP282674 Acidianus bottle-shaped virus 3 strain ABV3, complete genome.: CUDA out of memory. Tried to allocate 3.26 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.83 GiB is free. Including non-PyTorch memory, this process has 12.76 GiB memory in use. Of the allocated memory 6.54 GiB is allocated by PyTorch, and 3.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 3043/3316 [1:06:56<10:41,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence KC139521 Salmonella phage FSL SP-004, complete genome.: CUDA out of memory. Tried to allocate 3.40 GiB. GPU 0 has a total capacity of 15.60 GiB of which 2.27 GiB is free. Including non-PyTorch memory, this process has 13.32 GiB memory in use. Of the allocated memory 6.69 GiB is allocated by PyTorch, and 4.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 3049/3316 [1:07:06<06:56,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence HQ110084 Erwinia phage ENT90, complete genome.: CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 15.60 GiB of which 3.33 GiB is free. Including non-PyTorch memory, this process has 12.27 GiB memory in use. Of the allocated memory 6.39 GiB is allocated by PyTorch, and 3.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3316/3316 [1:12:44<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3300 sequences\n",
      "Embeddings array shape: (3300, 1920)\n",
      "Number of headers: 3300\n",
      "Failed to process 16 sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all sequences in the FASTA file and extract embeddings\n",
    "\n",
    "# Create lists to store results and failed sequences\n",
    "results = []\n",
    "failed_sequences = []\n",
    "\n",
    "# Process each sequence\n",
    "for record in tqdm(seq_subset_30k):\n",
    "    try:\n",
    "        # Extract sequence and header\n",
    "        sequence = str(record.seq)\n",
    "        header = record.description\n",
    "\n",
    "        # Make sure GPU memory is cleared before processing\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Tokenize and get embeddings\n",
    "        input_ids = (\n",
    "            torch.tensor(\n",
    "                evo2_model.tokenizer.tokenize(sequence),\n",
    "                dtype=torch.int,\n",
    "            )\n",
    "            .unsqueeze(0)\n",
    "            .to(\"cuda:0\")\n",
    "        )\n",
    "\n",
    "        layer_name = \"blocks.24.mlp.l3\"\n",
    "\n",
    "        # Get embeddings with explicit dtype to avoid BFloat16 issues\n",
    "        with torch.amp.autocast(\"cuda\", enabled=False):\n",
    "            outputs, embeddings = evo2_model(\n",
    "                input_ids, return_embeddings=True, layer_names=[layer_name]\n",
    "            )\n",
    "\n",
    "        # Extract the embeddings tensor and ensure it's float32\n",
    "        embedding_tensor = embeddings[layer_name].to(torch.float32)\n",
    "\n",
    "        # Average over the sequence length dimension to get a 1920-dim vector\n",
    "        # Shape goes from [1, n, 1920] to [1, 1920] to [1920]\n",
    "        avg_embedding = embedding_tensor.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "        # Store results\n",
    "        results.append({\"header\": header, \"embedding\": avg_embedding})\n",
    "\n",
    "        # Clear GPU cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sequence {header}: {e}\")\n",
    "        # Record the failed sequence\n",
    "        failed_sequences.append(header)\n",
    "        # Force GPU memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "# Extract headers\n",
    "headers = [r[\"header\"] for r in results]\n",
    "\n",
    "# Create a numpy array of all embeddings\n",
    "embeddings_array = np.stack([r[\"embedding\"] for r in results])\n",
    "\n",
    "print(f\"Processed {len(results)} sequences\")\n",
    "print(f\"Embeddings array shape: {embeddings_array.shape}\")\n",
    "print(f\"Number of headers: {len(headers)}\")\n",
    "\n",
    "# Save embeddings and headers with model-specific names\n",
    "np.save(\"../results/embeddings_evo2.npy\", embeddings_array)\n",
    "pd.DataFrame({\"header\": headers}).to_csv(\"../results/headers_evo2.csv\", index=False)\n",
    "\n",
    "# Save failed sequences\n",
    "if failed_sequences:\n",
    "    print(f\"Failed to process {len(failed_sequences)} sequences\")\n",
    "    pd.DataFrame({\"header\": failed_sequences}).to_csv(\n",
    "        \"../results/failed_sequences_evo2.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
