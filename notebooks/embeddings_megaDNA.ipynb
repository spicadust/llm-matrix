{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME: /run/current-system/sw\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Python version: 3.12.7 (main, Oct 16 2024, 04:37:19) [Clang 18.1.8 ]\n",
      "\n",
      "CUDA available: True\n",
      "\n",
      "CUDA Device Details:\n",
      "  Device: NVIDIA GeForce RTX 4060 Ti\n",
      "  Total memory: 15.60 GB\n",
      "  CUDA capability: 8.9\n",
      "  Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()  # will load from .env file in the same directory\n",
    "\n",
    "# Then add this to check the environment:\n",
    "\n",
    "print(f\"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not set')}\")\n",
    "\n",
    "\n",
    "def check_cuda():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "\n",
    "    # Check if CUDA is available\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"\\nCUDA available: {cuda_available}\")\n",
    "\n",
    "    if cuda_available:\n",
    "        # Get current CUDA device\n",
    "        current_device = torch.cuda.current_device()\n",
    "        # Get device properties\n",
    "        device_props = torch.cuda.get_device_properties(current_device)\n",
    "\n",
    "        print(\"\\nCUDA Device Details:\")\n",
    "        print(f\"  Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "        print(f\"  Total memory: {device_props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  CUDA capability: {device_props.major}.{device_props.minor}\")\n",
    "        print(f\"  Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"\\nNo CUDA devices available\")\n",
    "\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spica/Repos/megaDNA_matrix/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "# model can be downloaded from https://huggingface.co/lingxusb/megaDNA_updated/resolve/main/megaDNA_phage_145M.pt\n",
    "model_path = \"../checkpoints/megaDNA_phage_277M.pt\"  # model name\n",
    "device = \"cuda\"  # change this to 'cuda' if you use GPU\n",
    "\n",
    "model = torch.load(model_path, map_location=torch.device(device), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "sequences = list(SeqIO.parse(\"../dataset/strain_131k.fasta\", \"fasta\"))\n",
    "# metadata_cleaned = pd.read_csv(\"../dataset/metadata_cleaned.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sequence, model, device, max_seq_length=131072):\n",
    "    \"\"\"Get embeddings for a single sequence\n",
    "    Args:\n",
    "        sequence: Input DNA sequence\n",
    "        model: MEGADNA model\n",
    "        device: torch device (CPU/GPU)\n",
    "        max_seq_length: Maximum sequence length (default: 131072 from model dimensions 128*64*16)\n",
    "    Returns:\n",
    "        combined_emb: 964-dimensional embedding vector (on GPU)\n",
    "    \"\"\"\n",
    "    # Check sequence length\n",
    "    if len(sequence) > max_seq_length:\n",
    "        print(f\"Warning: Sequence length {len(sequence)} exceeds max length {max_seq_length}\")\n",
    "        sequence = sequence[:max_seq_length]\n",
    "\n",
    "    # Encode sequence\n",
    "    nt_vocab = [\"**\", \"A\", \"T\", \"C\", \"G\", \"#\"]\n",
    "    encoded = [0]  # Start token\n",
    "    for nucleotide in str(sequence):\n",
    "        if nucleotide in nt_vocab:\n",
    "            encoded.append(nt_vocab.index(nucleotide))\n",
    "        else:\n",
    "            encoded.append(1)\n",
    "    encoded.append(5)  # End token\n",
    "\n",
    "    input_seq = torch.tensor(encoded).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input_seq, return_value=\"embedding\")\n",
    "        local_emb = torch.mean(embeddings[0].squeeze(), dim=0)  # (512,)\n",
    "        middle_emb = torch.mean(torch.mean(embeddings[1].squeeze(), dim=0), dim=0)  # (256,)\n",
    "        global_emb = torch.mean(torch.mean(embeddings[2].squeeze(), dim=0), dim=0)  # (196,)\n",
    "        combined_emb = torch.cat([local_emb, middle_emb, global_emb])  # (964,)\n",
    "\n",
    "    return combined_emb\n",
    "\n",
    "\n",
    "def process_sequences(sequences, model, device, batch_size=50, max_seq_length=131072):\n",
    "    \"\"\"Process multiple sequences in batches\n",
    "    Args:\n",
    "        sequences: List of sequences to process\n",
    "        model: MEGADNA model\n",
    "        device: torch device (CPU/GPU)\n",
    "        batch_size: Number of sequences to process at once\n",
    "        max_seq_length: Maximum sequence length (default: 131072 from model dimensions 128*64*16)\n",
    "    Returns:\n",
    "        all_embeddings: Tensor of shape (n_sequences, 964) containing embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = torch.zeros((len(sequences), 1400), device=device)\n",
    "\n",
    "    # Create progress bar for all sequences\n",
    "    with tqdm(total=len(sequences), desc=\"Processing sequences\") as pbar:\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch = sequences[i : i + batch_size]\n",
    "\n",
    "            for j, record in enumerate(batch):\n",
    "                try:\n",
    "                    all_embeddings[i + j] = get_embeddings(\n",
    "                        record.seq, model, device, max_seq_length\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sequence {record.id}: {str(e)}\")\n",
    "\n",
    "                pbar.update(1)  # Update progress bar for each sequence\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|██████████| 3238/3238 [10:40<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final embeddings shape: torch.Size([3238, 1400])\n"
     ]
    }
   ],
   "source": [
    "# Process all sequences\n",
    "embeddings = process_sequences(\n",
    "    sequences=sequences, model=model, device=device, batch_size=128, max_seq_length=131072\n",
    ")\n",
    "\n",
    "print(\"\\nFinal embeddings shape:\", embeddings.shape)  # Should be (number of sequences, 964)\n",
    "\n",
    "# Save embeddings and sequence information\n",
    "embeddings_np = embeddings.cpu().numpy()\n",
    "headers = [record.description for record in sequences]\n",
    "np.save(\"../results/embeddings_megaDNA277_strain.npy\", embeddings_np)\n",
    "np.save(\"../results/headers_megaDNA277_strain.npy\", np.array(headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering based on embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "embeddings = np.load(\"../results/embeddings_megaDNA277_millard.npy\")\n",
    "headers = np.load(\"../results/headers_megaDNA277_millard.npy\")\n",
    "metadata_cleaned = pd.read_csv(\"../dataset/metadata_millard_cleaned.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Create dataframe with metadata\n",
    "accessions = [header.split()[0] for header in headers]\n",
    "df = pd.DataFrame({\"Accession\": accessions})\n",
    "df = df.merge(metadata_cleaned[[\"Accession\", \"Genus\", \"Family\", \"Host\"]], on=\"Accession\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(clusters, df, method_name, n_components, explained_var, output_file):\n",
    "    def calculate_purity(cluster_labels, taxonomy_labels):\n",
    "        total = len(cluster_labels)\n",
    "        purity = 0\n",
    "        for cluster in np.unique(cluster_labels):\n",
    "            cluster_mask = cluster_labels == cluster\n",
    "            if sum(cluster_mask) > 0:\n",
    "                most_common = pd.Series(taxonomy_labels[cluster_mask]).mode()[0]\n",
    "                purity += sum(taxonomy_labels[cluster_mask] == most_common)\n",
    "        return purity / total\n",
    "\n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(f\"\\n{'=' * 50}\\n\")\n",
    "        f.write(f\"Clustering Method: {method_name}\\n\")\n",
    "        f.write(f\"PCA dimensions: {n_components} (explained variance: {explained_var:.3f})\\n\")\n",
    "        f.write(f\"{'=' * 50}\\n\\n\")\n",
    "\n",
    "        # Analyze each cluster\n",
    "        for cluster in sorted(np.unique(clusters)):\n",
    "            cluster_mask = clusters == cluster\n",
    "            cluster_size = sum(cluster_mask)\n",
    "\n",
    "            f.write(f\"Cluster {cluster} (Size: {cluster_size})\\n\\n\")\n",
    "\n",
    "            # Top Genera\n",
    "            f.write(\"Top Genera:\\n\")\n",
    "            genera_counts = df[cluster_mask][\"Genus\"].value_counts().head()\n",
    "            f.write(genera_counts.to_string())\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            # Top Families\n",
    "            f.write(\"Top Families:\\n\")\n",
    "            family_counts = df[cluster_mask][\"Family\"].value_counts().head()\n",
    "            f.write(family_counts.to_string())\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            # Top Hosts\n",
    "            f.write(\"Top Hosts:\\n\")\n",
    "            host_counts = df[cluster_mask][\"Host\"].value_counts().head()\n",
    "            f.write(host_counts.to_string())\n",
    "            f.write(\"\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "        # Calculate purity scores\n",
    "        genus_purity = calculate_purity(clusters, df[\"Genus\"])\n",
    "        family_purity = calculate_purity(clusters, df[\"Family\"])\n",
    "        host_purity = calculate_purity(clusters, df[\"Host\"])\n",
    "\n",
    "        f.write(\"\\nCluster Purity Scores:\\n\")\n",
    "        f.write(f\"Genus Purity: {genus_purity:.3f}\\n\")\n",
    "        f.write(f\"Family Purity: {family_purity:.3f}\\n\")\n",
    "        f.write(f\"Host Purity: {host_purity:.3f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA dimensions to try\n",
    "pca_dimensions = [2, 10, 20, 50, 100, 200, 500, 964]  # 964 is original dimension\n",
    "\n",
    "# Numbers of clusters to try - MODIFY THIS LIST TO CHANGE CLUSTER NUMBERS\n",
    "n_clusters_list = [100, 200, 300, 500]  # Example: wider range of clusters\n",
    "\n",
    "# DBSCAN parameters\n",
    "dbscan_params = {\"eps\": 5, \"min_samples\": 5}\n",
    "\n",
    "output_file = \"../results/clustering_analysis_megaDNA277.txt\"\n",
    "\n",
    "# Clear file\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Clustering Analysis Results with Different PCA Dimensions and Cluster Numbers\\n\\n\")\n",
    "\n",
    "# Try each PCA dimension\n",
    "for n_components in pca_dimensions:\n",
    "    if n_components < 964:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        embeddings_reduced = pca.fit_transform(embeddings_scaled)\n",
    "        explained_var = sum(pca.explained_variance_ratio_)\n",
    "    else:\n",
    "        embeddings_reduced = embeddings_scaled\n",
    "        explained_var = 1.0\n",
    "\n",
    "    # Try different numbers of clusters\n",
    "    for n_clusters in n_clusters_list:\n",
    "        # K-means\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(embeddings_reduced)\n",
    "        analyze_clusters(\n",
    "            clusters,\n",
    "            df,\n",
    "            f\"K-means (n_clusters={n_clusters})\",\n",
    "            n_components,\n",
    "            explained_var,\n",
    "            output_file,\n",
    "        )\n",
    "\n",
    "        # Hierarchical\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        clusters = hierarchical.fit_predict(embeddings_reduced)\n",
    "        analyze_clusters(\n",
    "            clusters,\n",
    "            df,\n",
    "            f\"Hierarchical (n_clusters={n_clusters})\",\n",
    "            n_components,\n",
    "            explained_var,\n",
    "            output_file,\n",
    "        )\n",
    "\n",
    "    # DBSCAN\n",
    "    dbscan = DBSCAN(**dbscan_params)  # type: ignore\n",
    "    clusters = dbscan.fit_predict(embeddings_reduced)\n",
    "    analyze_clusters(\n",
    "        clusters,\n",
    "        df,\n",
    "        f\"DBSCAN (eps={dbscan_params['eps']}, min_samples={dbscan_params['min_samples']})\",\n",
    "        n_components,\n",
    "        explained_var,\n",
    "        output_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### protein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1375 common accessions between datasets\n",
      "Using subset of 500 sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating protein distances:   2%|▏         | 2784/124750 [00:51<19:01, 106.86it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Bio import Phylo\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "embeddings = np.load(\"../results/embeddings_megaDNA277_millard.npy\")\n",
    "headers = np.load(\"../results/headers_megaDNA277_millard.npy\")\n",
    "accessions = [header.split()[0] for header in headers]\n",
    "protein_tree = Phylo.read(\"../dataset/set.bionj.newick\", \"newick\")  # type: ignore\n",
    "tree_terminals = [terminal.name for terminal in protein_tree.get_terminals()]\n",
    "\n",
    "# Find common accessions\n",
    "common_accessions = []\n",
    "accession_indices = []\n",
    "tree_names = []\n",
    "\n",
    "for i, acc in enumerate(accessions):\n",
    "    if acc in tree_terminals:\n",
    "        common_accessions.append(acc)\n",
    "        accession_indices.append(i)\n",
    "        tree_names.append(acc)\n",
    "\n",
    "n = len(common_accessions)\n",
    "print(f\"Found {n} common accessions between datasets\")\n",
    "\n",
    "# Use smaller subset for testing\n",
    "use_subset = True  # Set to False to use all data\n",
    "if use_subset:\n",
    "    subset_size = 500\n",
    "    if n > subset_size:\n",
    "        indices = np.random.choice(n, subset_size, replace=False)\n",
    "        common_accessions = [common_accessions[i] for i in indices]\n",
    "        accession_indices = [accession_indices[i] for i in indices]\n",
    "        tree_names = [tree_names[i] for i in indices]\n",
    "        n = subset_size\n",
    "        print(f\"Using subset of {n} sequences\")\n",
    "\n",
    "# Calculate distances\n",
    "embeddings_subset = embeddings[accession_indices]\n",
    "dna_distances = pdist(embeddings_subset, metric=\"euclidean\")\n",
    "dna_distances = squareform(dna_distances)\n",
    "\n",
    "# Get protein-based distances from tree with progress bar\n",
    "protein_distances = np.zeros((n, n))\n",
    "total_pairs = (n * (n - 1)) // 2\n",
    "\n",
    "with tqdm(total=total_pairs, desc=\"Calculating protein distances\") as pbar:\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dist = protein_tree.distance(\n",
    "                protein_tree.find_any(name=tree_names[i]), protein_tree.find_any(name=tree_names[j])\n",
    "            )\n",
    "            protein_distances[i, j] = dist\n",
    "            protein_distances[j, i] = dist\n",
    "            pbar.update(1)\n",
    "\n",
    "# Normalize distances\n",
    "print(\"Normalizing distances...\")\n",
    "dna_dist_normalized = (dna_distances - dna_distances.min()) / (\n",
    "    dna_distances.max() - dna_distances.min()\n",
    ")\n",
    "protein_dist_normalized = (protein_distances - protein_distances.min()) / (\n",
    "    protein_distances.max() - protein_distances.min()\n",
    ")\n",
    "\n",
    "difference = dna_dist_normalized - protein_dist_normalized\n",
    "\n",
    "# Get top pairs\n",
    "n_pairs = 10\n",
    "print(\"Finding top pairs...\")\n",
    "pairs = []\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        pairs.append((i, j, difference[i, j]))\n",
    "\n",
    "pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\nTop pairs with different DNA but similar proteins:\")\n",
    "for i, j, diff in pairs[:n_pairs]:\n",
    "    print(f\"\\nPair: {common_accessions[i]} - {common_accessions[j]}\")\n",
    "    print(f\"DNA distance: {dna_distances[i, j]:.3f}\")\n",
    "    print(f\"Protein distance: {protein_distances[i, j]:.3f}\")\n",
    "    print(f\"Difference score: {diff:.3f}\")\n",
    "\n",
    "# Visualize correlation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(dna_dist_normalized.flatten(), protein_dist_normalized.flatten(), alpha=0.1)\n",
    "plt.xlabel(\"DNA-based Distance (normalized)\")\n",
    "plt.ylabel(\"Protein-based Distance (normalized)\")\n",
    "plt.title(f\"Correlation between DNA and Protein Distances (n={n})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
